<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3 Welcome to Probability | Psychology Research Methods</title>
  <meta name="description" content="Psy 310: Research Methods" />
  <meta name="generator" content="bookdown 0.44 and GitBook 2.6.7" />

  <meta property="og:title" content="3 Welcome to Probability | Psychology Research Methods" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="https://psychmethods.github.io/coursenotes/assets/logo.png" />
  <meta property="og:description" content="Psy 310: Research Methods" />
  <meta name="github-repo" content="psychmethods/coursenotes" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3 Welcome to Probability | Psychology Research Methods" />
  <meta name="twitter:site" content="@smasongarrison" />
  <meta name="twitter:description" content="Psy 310: Research Methods" />
  <meta name="twitter:image" content="https://psychmethods.github.io/coursenotes/assets/logo.png" />

<meta name="author" content="S. Mason Garrison" />


<meta name="date" content="2025-09-17" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  <link rel="shortcut icon" href="assets/favicon.ico" type="image/x-icon" />
<link rel="prev" href="welcome-to-univariate-statistics.html"/>
<link rel="next" href="probability.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<link href="libs/font-awesome-6.5.2/css/all.min.css" rel="stylesheet" />
<link href="libs/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet" />
<link href="libs/vembedr-0.1.5/css/vembedr.css" rel="stylesheet" />
<link href="libs/htmltools-fill-0.5.8.1/fill.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.6.4/htmlwidgets.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.34.0/datatables.js"></script>
<link href="libs/dt-core-1.13.6/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.13.6/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.13.6/js/jquery.dataTables.min.js"></script>
<link href="libs/nouislider-7.0.10/jquery.nouislider.min.css" rel="stylesheet" />
<script src="libs/nouislider-7.0.10/jquery.nouislider.min.js"></script>
<link href="libs/selectize-0.12.0/selectize.bootstrap3.css" rel="stylesheet" />
<script src="libs/selectize-0.12.0/selectize.min.js"></script>
<link href="libs/crosstalk-1.2.2/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk-1.2.2/js/crosstalk.min.js"></script>
<script src="libs/twitter-widget-0.0.1/widgets.js"></script>
<html>

  <head>
  <script src="anchor-links.js"></script>

  <!-- Global site tag (gtag.js) - Google Analytics
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-68219208-1"></script>

  <script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-68219208-1');
  </script>
   -->
  </head>

</html>


<style type="text/css">
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="assets/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="toc-logo"><a href="./"><img src="assets/logo.png" style="border-radius: 00%;" ></a></li>

<li class="divider"></li>
<li class="part"><span><b>Front Matter</b></span></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome to PSY 310</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#mason-notes"><i class="fa fa-check"></i>Mason Notes</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#how-to-use-these-notes"><i class="fa fa-check"></i>How to use these notes</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="attribution.html"><a href="attribution.html"><i class="fa fa-check"></i>Attribution</a>
<ul>
<li class="chapter" data-level="" data-path="attribution.html"><a href="attribution.html#major-attributions"><i class="fa fa-check"></i>Major Attributions</a></li>
<li class="chapter" data-level="" data-path="attribution.html"><a href="attribution.html#additional-attributions"><i class="fa fa-check"></i>Additional Attributions</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="license.html"><a href="license.html"><i class="fa fa-check"></i>License</a></li>
<li class="chapter" data-level="" data-path="colophon.html"><a href="colophon.html"><i class="fa fa-check"></i>Colophon</a></li>
<li class="part"><span><b>I Module 00</b></span></li>
<li class="chapter" data-level="" data-path="dont-miss-module-00.html"><a href="dont-miss-module-00.html"><i class="fa fa-check"></i>Don’t Miss Module 00</a>
<ul>
<li class="chapter" data-level="0.1" data-path="dont-miss-module-00.html"><a href="dont-miss-module-00.html#learning-goals-for-this-module-chapter-0"><i class="fa fa-check"></i><b>0.1</b> Learning Goals for this Module (Chapter 0)</a></li>
<li class="chapter" data-level="0.2" data-path="dont-miss-module-00.html"><a href="dont-miss-module-00.html#course-modality"><i class="fa fa-check"></i><b>0.2</b> Course Modality</a>
<ul>
<li class="chapter" data-level="0.2.1" data-path="dont-miss-module-00.html"><a href="dont-miss-module-00.html#successful-asynchronous-learning"><i class="fa fa-check"></i><b>0.2.1</b> Successful Asynchronous Learning</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="1" data-path="knowledge-is-power.html"><a href="knowledge-is-power.html"><i class="fa fa-check"></i><b>1</b> Knowledge is Power</a>
<ul>
<li class="chapter" data-level="1.1" data-path="knowledge-is-power.html"><a href="knowledge-is-power.html#meet-prof.-mason"><i class="fa fa-check"></i><b>1.1</b> Meet Prof. Mason</a></li>
<li class="chapter" data-level="1.2" data-path="knowledge-is-power.html"><a href="knowledge-is-power.html#website-tour"><i class="fa fa-check"></i><b>1.2</b> Website Tour</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="guidance.html"><a href="guidance.html"><i class="fa fa-check"></i>Guidance</a>
<ul>
<li class="chapter" data-level="1.3" data-path="guidance.html"><a href="guidance.html#syllabus"><i class="fa fa-check"></i><b>1.3</b> Syllabus</a></li>
<li class="chapter" data-level="1.4" data-path="guidance.html"><a href="guidance.html#materials"><i class="fa fa-check"></i><b>1.4</b> Materials</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="guidance.html"><a href="guidance.html#hardware"><i class="fa fa-check"></i><b>1.4.1</b> Hardware</a></li>
<li class="chapter" data-level="1.4.2" data-path="guidance.html"><a href="guidance.html#required-texts"><i class="fa fa-check"></i><b>1.4.2</b> Required Texts</a></li>
<li class="chapter" data-level="1.4.3" data-path="guidance.html"><a href="guidance.html#software"><i class="fa fa-check"></i><b>1.4.3</b> Software</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Module 04</b></span></li>
<li class="chapter" data-level="2" data-path="welcome-to-univariate-statistics.html"><a href="welcome-to-univariate-statistics.html"><i class="fa fa-check"></i><b>2</b> Welcome to Univariate Statistics</a>
<ul>
<li class="chapter" data-level="2.1" data-path="welcome-to-univariate-statistics.html"><a href="welcome-to-univariate-statistics.html#learning-goals"><i class="fa fa-check"></i><b>2.1</b> Learning Goals</a></li>
<li class="chapter" data-level="2.2" data-path="welcome-to-univariate-statistics.html"><a href="welcome-to-univariate-statistics.html#degrees-of-freedom-and-bessels-correction"><i class="fa fa-check"></i><b>2.2</b> Degrees of Freedom and Bessel’s Correction</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="welcome-to-univariate-statistics.html"><a href="welcome-to-univariate-statistics.html#why-degrees-of-freedom-exist"><i class="fa fa-check"></i><b>2.2.1</b> Why degrees of freedom exist</a></li>
<li class="chapter" data-level="2.2.2" data-path="welcome-to-univariate-statistics.html"><a href="welcome-to-univariate-statistics.html#bessels-correction"><i class="fa fa-check"></i><b>2.2.2</b> Bessel’s correction</a></li>
<li class="chapter" data-level="2.2.3" data-path="welcome-to-univariate-statistics.html"><a href="welcome-to-univariate-statistics.html#a-quick-simulation"><i class="fa fa-check"></i><b>2.2.3</b> A quick simulation</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Module 06</b></span></li>
<li class="chapter" data-level="3" data-path="welcome-to-probability.html"><a href="welcome-to-probability.html"><i class="fa fa-check"></i><b>3</b> Welcome to Probability</a>
<ul>
<li class="chapter" data-level="3.1" data-path="welcome-to-probability.html"><a href="welcome-to-probability.html#basic-probability-concepts"><i class="fa fa-check"></i><b>3.1</b> Basic Probability Concepts</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="welcome-to-probability.html"><a href="welcome-to-probability.html#what-is-probability"><i class="fa fa-check"></i><b>3.1.1</b> What is probability?</a></li>
<li class="chapter" data-level="3.1.2" data-path="welcome-to-probability.html"><a href="welcome-to-probability.html#the-probability-formula"><i class="fa fa-check"></i><b>3.1.2</b> The Probability Formula</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="welcome-to-probability.html"><a href="welcome-to-probability.html#probability-frequency-distributions"><i class="fa fa-check"></i><b>3.2</b> Probability &amp; Frequency Distributions</a></li>
<li class="chapter" data-level="3.3" data-path="welcome-to-probability.html"><a href="welcome-to-probability.html#probability-in-normal-distributions"><i class="fa fa-check"></i><b>3.3</b> Probability in Normal Distributions</a></li>
<li class="chapter" data-level="3.4" data-path="welcome-to-probability.html"><a href="welcome-to-probability.html#considerations-in-understanding-probability-and-inferential-statistics-sampling"><i class="fa fa-check"></i><b>3.4</b> Considerations in understanding probability and inferential statistics: sampling</a></li>
<li class="chapter" data-level="3.5" data-path="welcome-to-probability.html"><a href="welcome-to-probability.html#more-probability-terms"><i class="fa fa-check"></i><b>3.5</b> More Probability Terms</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="welcome-to-probability.html"><a href="welcome-to-probability.html#statistical-independence"><i class="fa fa-check"></i><b>3.5.1</b> Statistical Independence</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="welcome-to-probability.html"><a href="welcome-to-probability.html#recap"><i class="fa fa-check"></i><b>3.6</b> Recap</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="welcome-to-probability.html"><a href="welcome-to-probability.html#learning-objectives"><i class="fa fa-check"></i><b>3.6.1</b> Learning objectives</a></li>
<li class="chapter" data-level="3.6.2" data-path="welcome-to-probability.html"><a href="welcome-to-probability.html#exercises"><i class="fa fa-check"></i><b>3.6.2</b> Exercises</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IV Module 07</b></span></li>
<li class="chapter" data-level="4" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>4</b> Welcome to Sampling and some more on probability</a>
<ul>
<li class="chapter" data-level="4.1" data-path="probability.html"><a href="probability.html#probstats"><i class="fa fa-check"></i><b>4.1</b> How are probability and statistics different?</a></li>
<li class="chapter" data-level="4.2" data-path="probability.html"><a href="probability.html#probmeaning"><i class="fa fa-check"></i><b>4.2</b> What does probability mean?</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="probability.html"><a href="probability.html#the-frequentist-view"><i class="fa fa-check"></i><b>4.2.1</b> The frequentist view</a></li>
<li class="chapter" data-level="4.2.2" data-path="probability.html"><a href="probability.html#the-bayesian-view"><i class="fa fa-check"></i><b>4.2.2</b> The Bayesian view</a></li>
<li class="chapter" data-level="4.2.3" data-path="probability.html"><a href="probability.html#whats-the-difference-and-who-is-right"><i class="fa fa-check"></i><b>4.2.3</b> What’s the difference? And who is right?</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="probability.html"><a href="probability.html#basicprobability"><i class="fa fa-check"></i><b>4.3</b> Basic probability theory</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="probability.html"><a href="probability.html#introducing-probability-distributions"><i class="fa fa-check"></i><b>4.3.1</b> Introducing probability distributions</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="probability.html"><a href="probability.html#binomial"><i class="fa fa-check"></i><b>4.4</b> The binomial distribution</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="probability.html"><a href="probability.html#introducing-the-binomial"><i class="fa fa-check"></i><b>4.4.1</b> Introducing the binomial</a></li>
<li class="chapter" data-level="4.4.2" data-path="probability.html"><a href="probability.html#working-with-the-binomial-distribution-in-r"><i class="fa fa-check"></i><b>4.4.2</b> Working with the binomial distribution in R</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="probability.html"><a href="probability.html#normal"><i class="fa fa-check"></i><b>4.5</b> The normal distribution</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="probability.html"><a href="probability.html#density"><i class="fa fa-check"></i><b>4.5.1</b> Probability density</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="probability.html"><a href="probability.html#otherdists"><i class="fa fa-check"></i><b>4.6</b> Other useful distributions</a></li>
<li class="chapter" data-level="4.7" data-path="probability.html"><a href="probability.html#summary"><i class="fa fa-check"></i><b>4.7</b> Summary</a></li>
</ul></li>
<li class="part"><span><b>V Other Coolness</b></span></li>
<li class="chapter" data-level="5" data-path="good-resources.html"><a href="good-resources.html"><i class="fa fa-check"></i><b>5</b> Good Resources</a></li>
<li class="chapter" data-level="6" data-path="media-without-a-home-yet.html"><a href="media-without-a-home-yet.html"><i class="fa fa-check"></i><b>6</b> Media without a home yet</a>
<ul>
<li class="chapter" data-level="6.1" data-path="media-without-a-home-yet.html"><a href="media-without-a-home-yet.html#visualizing-linear-models-an-r-bag-of-tricks"><i class="fa fa-check"></i><b>6.1</b> Visualizing Linear Models: An R Bag of Tricks</a></li>
<li class="chapter" data-level="6.2" data-path="media-without-a-home-yet.html"><a href="media-without-a-home-yet.html#for-new-programmers-learning-keyboard-shortcuts"><i class="fa fa-check"></i><b>6.2</b> For new programmers learning keyboard shortcuts…</a></li>
<li class="chapter" data-level="6.3" data-path="media-without-a-home-yet.html"><a href="media-without-a-home-yet.html#are-you-a-student-if-yes-this-is-the-best-data-science-project-for-you"><i class="fa fa-check"></i><b>6.3</b> Are you a student? If yes, this is the best data science project for you!</a></li>
<li class="chapter" data-level="6.4" data-path="media-without-a-home-yet.html"><a href="media-without-a-home-yet.html#rstudio-is-magic"><i class="fa fa-check"></i><b>6.4</b> rstudio is magic</a></li>
<li class="chapter" data-level="6.5" data-path="media-without-a-home-yet.html"><a href="media-without-a-home-yet.html#automation-quote"><i class="fa fa-check"></i><b>6.5</b> automation quote</a></li>
<li class="chapter" data-level="6.6" data-path="media-without-a-home-yet.html"><a href="media-without-a-home-yet.html#how-computer-memory-works"><i class="fa fa-check"></i><b>6.6</b> How computer memory works!</a></li>
<li class="chapter" data-level="6.7" data-path="media-without-a-home-yet.html"><a href="media-without-a-home-yet.html#is-coding-a-math-skill-or-a-language-skill-neither-both"><i class="fa fa-check"></i><b>6.7</b> Is Coding a Math Skill or a Language Skill? Neither? Both?</a></li>
<li class="chapter" data-level="6.8" data-path="media-without-a-home-yet.html"><a href="media-without-a-home-yet.html#quantum-computers-explained"><i class="fa fa-check"></i><b>6.8</b> Quantum Computers Explained!</a></li>
<li class="chapter" data-level="6.9" data-path="media-without-a-home-yet.html"><a href="media-without-a-home-yet.html#the-rise-of-the-machines-why-automation-is-different-this-time"><i class="fa fa-check"></i><b>6.9</b> The Rise of the Machines – Why Automation is Different this Time</a></li>
<li class="chapter" data-level="6.10" data-path="media-without-a-home-yet.html"><a href="media-without-a-home-yet.html#who-would-be-king-of-america-if-george-washington-had-been-made-a-monarch"><i class="fa fa-check"></i><b>6.10</b> Who Would Be King of America if George Washington had been made a monarch?</a></li>
<li class="chapter" data-level="6.11" data-path="media-without-a-home-yet.html"><a href="media-without-a-home-yet.html#emergence-how-stupid-things-become-smart-together"><i class="fa fa-check"></i><b>6.11</b> Emergence – How Stupid Things Become Smart Together</a></li>
<li class="chapter" data-level="6.12" data-path="media-without-a-home-yet.html"><a href="media-without-a-home-yet.html#the-birthday-paradox"><i class="fa fa-check"></i><b>6.12</b> The Birthday Paradox</a></li>
<li class="chapter" data-level="6.13" data-path="media-without-a-home-yet.html"><a href="media-without-a-home-yet.html#why-cant-you-divide-by-zero"><i class="fa fa-check"></i><b>6.13</b> Why can’t you divide by zero?</a></li>
<li class="chapter" data-level="6.14" data-path="media-without-a-home-yet.html"><a href="media-without-a-home-yet.html#yea-hes-chewing-up-my-stats-homework-but-that-face-though"><i class="fa fa-check"></i><b>6.14</b> Yea he’s chewing up my stats homework but that face though…</a></li>
<li class="chapter" data-level="6.15" data-path="media-without-a-home-yet.html"><a href="media-without-a-home-yet.html#coding-kitty"><i class="fa fa-check"></i><b>6.15</b> Coding Kitty</a></li>
<li class="chapter" data-level="6.16" data-path="media-without-a-home-yet.html"><a href="media-without-a-home-yet.html#democratic-databases-science-on-github"><i class="fa fa-check"></i><b>6.16</b> Democratic databases: science on GitHub</a></li>
<li class="chapter" data-level="6.17" data-path="media-without-a-home-yet.html"><a href="media-without-a-home-yet.html#ten-simple-rules-for-getting-started-on-twitter-as-a-scientist"><i class="fa fa-check"></i><b>6.17</b> Ten simple rules for getting started on Twitter as a scientist</a></li>
<li class="chapter" data-level="6.18" data-path="media-without-a-home-yet.html"><a href="media-without-a-home-yet.html#nyt-data-ethics-stuff"><i class="fa fa-check"></i><b>6.18</b> NYT data ethics stuff</a></li>
<li class="chapter" data-level="6.19" data-path="media-without-a-home-yet.html"><a href="media-without-a-home-yet.html#section"><i class="fa fa-check"></i><b>6.19</b> </a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><center>
  <a rel="license" href="./license.html">
    License: CC-BY-SA<br>
    <i class = "fab fa-creative-commons fa-2x"></i>
    <i class = "fab fa-creative-commons-by fa-2x"></i>
    <i class = "fab fa-creative-commons-sa fa-2x"></i>
  </a>
</li></center>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Psychology Research Methods</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="welcome-to-probability" class="section level1 hasAnchor" number="3">
<h1><span class="header-section-number">3</span> Welcome to Probability<a href="welcome-to-probability.html#welcome-to-probability" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>As we transition from descriptive to inferential statistics, we begin our exploration of probability - the foundation of statistical inference. Probability provides the direct link between samples and the populations they represent. In this chapter, we’ll focus on the fundamental concepts needed to lay the groundwork for future inferential statistics, connecting probability to our existing knowledge of normal distributions and z-scores.</p>
<p><img src="https://imgs.xkcd.com/comics/probability.png" /><!-- --></p>
<p>Why do we care about probability? Well, besides helping you calculate your odds of winning the lottery (spoiler alert: they’re not good), probability is essential for understanding how likely our statistical results are to occur by chance. It’s like the bouncer at the club of statistical significance - deciding what gets in and what doesn’t.</p>
<div id="basic-probability-concepts" class="section level2 hasAnchor" number="3.1">
<h2><span class="header-section-number">3.1</span> Basic Probability Concepts<a href="welcome-to-probability.html#basic-probability-concepts" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Informally, we usually think of <strong>probability</strong> as a number that describes the likelihood of some event occurring, which ranges from zero (impossibility) to one (certainty). But probability isn’t always about precise numbers - sometimes it’s more like a weather forecast for your data.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="welcome-to-probability.html#cb4-1" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb4-2"><a href="welcome-to-probability.html#cb4-2" tabindex="-1"></a><span class="co"># labels</span></span>
<span id="cb4-3"><a href="welcome-to-probability.html#cb4-3" tabindex="-1"></a>terms_data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb4-4"><a href="welcome-to-probability.html#cb4-4" tabindex="-1"></a>  <span class="at">term =</span> <span class="fu">c</span>(<span class="st">&quot;Impossible&quot;</span>, <span class="st">&quot;Almost Impossible&quot;</span>, <span class="st">&quot;Very Unlikely&quot;</span>, <span class="st">&quot;Unlikely&quot;</span>, <span class="st">&quot;50-50 chance&quot;</span>, <span class="st">&quot;Likely&quot;</span>, <span class="st">&quot;Very Likely&quot;</span>, <span class="st">&quot;Almost Certain&quot;</span>, <span class="st">&quot;Absolute Certain&quot;</span>),</span>
<span id="cb4-5"><a href="welcome-to-probability.html#cb4-5" tabindex="-1"></a>  <span class="at">value =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.01</span>, <span class="fl">0.1</span>, <span class="fl">0.25</span>, <span class="fl">0.5</span>, <span class="fl">0.75</span>, <span class="fl">0.9</span>, <span class="fl">0.99</span>, <span class="dv">1</span>),</span>
<span id="cb4-6"><a href="welcome-to-probability.html#cb4-6" tabindex="-1"></a>  <span class="at">anchor =</span> <span class="fu">c</span>(T, F, F, F, T, F, F, F, T)</span>
<span id="cb4-7"><a href="welcome-to-probability.html#cb4-7" tabindex="-1"></a>) <span class="sc">%&gt;%</span> <span class="fu">arrange</span>(value)</span>
<span id="cb4-8"><a href="welcome-to-probability.html#cb4-8" tabindex="-1"></a><span class="co"># Set the factor levels according to the order of &#39;value&#39;</span></span>
<span id="cb4-9"><a href="welcome-to-probability.html#cb4-9" tabindex="-1"></a>terms_data<span class="sc">$</span>term <span class="ot">&lt;-</span> <span class="fu">factor</span>(terms_data<span class="sc">$</span>term, <span class="at">levels =</span> terms_data<span class="sc">$</span>term[<span class="fu">order</span>(terms_data<span class="sc">$</span>value)])</span>
<span id="cb4-10"><a href="welcome-to-probability.html#cb4-10" tabindex="-1"></a></span>
<span id="cb4-11"><a href="welcome-to-probability.html#cb4-11" tabindex="-1"></a>terms_data <span class="sc">%&gt;%</span> <span class="fu">filter</span>(anchor<span class="sc">==</span>T)<span class="sc">%&gt;%</span></span>
<span id="cb4-12"><a href="welcome-to-probability.html#cb4-12" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> value, <span class="at">y =</span> term)) <span class="sc">+</span></span>
<span id="cb4-13"><a href="welcome-to-probability.html#cb4-13" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">color =</span> anchor), <span class="at">size =</span> <span class="dv">3</span>) <span class="sc">+</span></span>
<span id="cb4-14"><a href="welcome-to-probability.html#cb4-14" tabindex="-1"></a>  <span class="fu">geom_segment</span>(<span class="fu">aes</span>(<span class="at">xend =</span> value, <span class="at">yend =</span> term), </span>
<span id="cb4-15"><a href="welcome-to-probability.html#cb4-15" tabindex="-1"></a>               <span class="at">x =</span> <span class="dv">0</span>, <span class="at">xend =</span> <span class="dv">1</span>, <span class="at">linetype =</span> <span class="st">&quot;dashed&quot;</span>) <span class="sc">+</span></span>
<span id="cb4-16"><a href="welcome-to-probability.html#cb4-16" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">&quot;red&quot;</span>, <span class="st">&quot;black&quot;</span>)) <span class="sc">+</span></span>
<span id="cb4-17"><a href="welcome-to-probability.html#cb4-17" tabindex="-1"></a>  <span class="fu">geom_text</span>(<span class="fu">aes</span>(<span class="at">label =</span> term), <span class="at">hjust =</span> .<span class="dv">5</span>, <span class="at">vjust =</span> <span class="sc">-</span>.<span class="dv">5</span>)<span class="sc">+</span></span>
<span id="cb4-18"><a href="welcome-to-probability.html#cb4-18" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb4-19"><a href="welcome-to-probability.html#cb4-19" tabindex="-1"></a>  <span class="fu">theme</span>(</span>
<span id="cb4-20"><a href="welcome-to-probability.html#cb4-20" tabindex="-1"></a>    <span class="at">axis.title =</span> <span class="fu">element_blank</span>(),</span>
<span id="cb4-21"><a href="welcome-to-probability.html#cb4-21" tabindex="-1"></a>    <span class="at">axis.text =</span> <span class="fu">element_blank</span>(),</span>
<span id="cb4-22"><a href="welcome-to-probability.html#cb4-22" tabindex="-1"></a>    <span class="at">axis.ticks =</span> <span class="fu">element_blank</span>(),</span>
<span id="cb4-23"><a href="welcome-to-probability.html#cb4-23" tabindex="-1"></a>    <span class="at">legend.position =</span> <span class="st">&quot;none&quot;</span></span>
<span id="cb4-24"><a href="welcome-to-probability.html#cb4-24" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb4-25"><a href="welcome-to-probability.html#cb4-25" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Probability Terminology&quot;</span>) <span class="sc">+</span></span>
<span id="cb4-26"><a href="welcome-to-probability.html#cb4-26" tabindex="-1"></a>  <span class="fu">xlim</span>(<span class="sc">-</span><span class="fl">0.1</span>, <span class="fl">1.1</span>)</span></code></pre></div>
<p><img src="0601_probability_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>Probability can be discussed more vaguely. “Chances are low it will rain today” is something you might hear from a meteorologist (or your weather app, if you’re not fancy enough to have a personal meteorologist). But notice how the probability changes with conditions: “Given there are no clouds, chances are low it will rain today.” That “given” is statistician-speak for “under these conditions.”</p>
<blockquote>
<p>Fun fact: The word “given” in probability is like the statistical version of “once upon a time” in fairy tales. It sets the stage for all the probabilistic action that follows!</p>
</blockquote>
<p>As conditions change, so does the probability. If it were cloudy and windy outside, we might say, “given the current weather conditions, there is a high probability that it is going to rain.” It’s like a game of probabilistic Jenga - change one piece, and the whole structure of likelihood shifts!</p>
<p>Now, you might have noticed that terms like “low” and “high” are pretty vague. They’re the statistical equivalent of saying “a pinch” in a recipe - it means something different to everyone. That’s why in statistics, we try to use more precise language or, even better, numbers to represent the probability of our event.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="welcome-to-probability.html#cb5-1" tabindex="-1"></a>terms_data <span class="sc">%&gt;%</span></span>
<span id="cb5-2"><a href="welcome-to-probability.html#cb5-2" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> value, <span class="at">y =</span> term)) <span class="sc">+</span></span>
<span id="cb5-3"><a href="welcome-to-probability.html#cb5-3" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">color =</span> anchor), <span class="at">size =</span> <span class="dv">3</span>) <span class="sc">+</span></span>
<span id="cb5-4"><a href="welcome-to-probability.html#cb5-4" tabindex="-1"></a>  <span class="fu">geom_segment</span>(<span class="fu">aes</span>(<span class="at">xend =</span> value, <span class="at">yend =</span> term), </span>
<span id="cb5-5"><a href="welcome-to-probability.html#cb5-5" tabindex="-1"></a>               <span class="at">x =</span> <span class="dv">0</span>, <span class="at">xend =</span> <span class="dv">1</span>, <span class="at">linetype =</span> <span class="st">&quot;dashed&quot;</span>) <span class="sc">+</span></span>
<span id="cb5-6"><a href="welcome-to-probability.html#cb5-6" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">&quot;red&quot;</span>, <span class="st">&quot;black&quot;</span>)) <span class="sc">+</span></span>
<span id="cb5-7"><a href="welcome-to-probability.html#cb5-7" tabindex="-1"></a>  <span class="fu">geom_text</span>(<span class="fu">aes</span>(<span class="at">label =</span> term), <span class="at">hjust =</span> .<span class="dv">5</span>, <span class="at">vjust =</span> <span class="sc">-</span>.<span class="dv">5</span>)<span class="sc">+</span></span>
<span id="cb5-8"><a href="welcome-to-probability.html#cb5-8" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb5-9"><a href="welcome-to-probability.html#cb5-9" tabindex="-1"></a>  <span class="fu">theme</span>(</span>
<span id="cb5-10"><a href="welcome-to-probability.html#cb5-10" tabindex="-1"></a>    <span class="at">axis.title =</span> <span class="fu">element_blank</span>(),</span>
<span id="cb5-11"><a href="welcome-to-probability.html#cb5-11" tabindex="-1"></a>    <span class="at">axis.text =</span> <span class="fu">element_blank</span>(),</span>
<span id="cb5-12"><a href="welcome-to-probability.html#cb5-12" tabindex="-1"></a>    <span class="at">axis.ticks =</span> <span class="fu">element_blank</span>(),</span>
<span id="cb5-13"><a href="welcome-to-probability.html#cb5-13" tabindex="-1"></a>    <span class="at">legend.position =</span> <span class="st">&quot;none&quot;</span></span>
<span id="cb5-14"><a href="welcome-to-probability.html#cb5-14" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb5-15"><a href="welcome-to-probability.html#cb5-15" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Probability Terminology&quot;</span>) <span class="sc">+</span></span>
<span id="cb5-16"><a href="welcome-to-probability.html#cb5-16" tabindex="-1"></a>  <span class="fu">xlim</span>(<span class="sc">-</span><span class="fl">0.1</span>, <span class="fl">1.1</span>)</span></code></pre></div>
<p><img src="0601_probability_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>Regardless of whether we’re using vague terms or precise numbers, the basic structure and logic of our probability statements remain consistent. It’s like the grammar of uncertainty - once you know the rules, you can speak probability fluently!</p>
<p>Sometimes probabilities will instead be expressed in percentages, which range from zero to one hundred, as when the weather forecast predicts a twenty percent chance of rain today. In each case, these numbers express how likely that particular event is, ranging from absolutely impossible (0%) to absolutely certain (100%). When we speak of the probability of something happening, we are talking about how likely a “thing” will happen based on the conditions present. To formalize probability theory, we first need to define a few terms:</p>
<div id="what-is-probability" class="section level3 hasAnchor" number="3.1.1">
<h3><span class="header-section-number">3.1.1</span> What is probability?<a href="welcome-to-probability.html#what-is-probability" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Probability theory</strong> is the branch of mathematics that deals with chance and uncertainty. It forms an important part of the foundation for statistics, because it provides us with the mathematical tools to describe uncertain events. The study of probability arose in part due to interest in understanding games of chance, like cards or dice. These games provide useful examples of many statistical concepts, because when we repeat these games the likelihood of different outcomes remains (mostly) the same.</p>
</div>
<div id="the-probability-formula" class="section level3 hasAnchor" number="3.1.2">
<h3><span class="header-section-number">3.1.2</span> The Probability Formula<a href="welcome-to-probability.html#the-probability-formula" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To calculate probability, we need an activity that produces or observes an outcome. This could be anything from flipping a coin to trying a new route to work. It is typically an experiment or any situation or activity in which the result is not known in advance. Before we dive into the formula, let’s define some key terms:</p>
<ul>
<li>’‘’Sample space’’’: This is the set of all possible outcomes for our activity. We write it using fancy squiggly brackets { }.
<ul>
<li>For a coin flip: {heads, tails}</li>
<li>For a six-sided die: {1, 2, 3, 4, 5, 6}</li>
<li>For the weather: {rain, not rain}</li>
</ul></li>
</ul>
<blockquote>
<p>For the amount of time it takes to get to work, the sample space is all possible real numbers greater than zero (since it can’t take a negative amount of time to get somewhere, at least not yet). We won’t bother writing out all those numbers within the brackets.</p>
</blockquote>
<ul>
<li>’‘’Outcome or event’’’: This is a subset of the sample space that we’re interested in. It’s the “what are we actually looking for?” part of our probability question. In principle, it could be one or more of possible outcomes in the sample space, but here we will focus primarily on elementary events which consist of exactly one possible outcome. An event is a catch-all term to talk about any specific thing happening.</li>
</ul>
<blockquote>
<p>For example, this could be it rains, obtaining heads in a single coin flip, rolling a 4 on a throw of the die, or taking 21 minutes to get home by the new route.</p>
</blockquote>
<p>In statistics, we usually define probability as the <strong>expected relative frequency of a particular outcome</strong>. The <strong>relative frequency</strong> is the number of times an event takes place relative to the number of times it could have taken place.</p>
<p>Let’s look at a slightly deeper example before we learn a basic probability formula. Say we have a regular, six-sided die (note that “die” is singular and “dice” is plural, a distinction that can be hard to get correct on the first try) and want to know how likely it is that we will roll a 1. That is, what is the probability of rolling a 1, given that the die is not weighted (which would introduce what we call a bias, though that is beyond the scope of this chapter). We could roll the die and see if it is a 1 or not, but that won’t tell us about the probability, it will only tell us a single result. We could also roll the die hundreds or thousands of times, recording each outcome and seeing what the final list looks like, but this is time-consuming, and rolling a die that many times may lead down a dark path to gambling or, worse, playing Dungeons &amp; Dragons. What we need is a simple equation that represents what we are looking for and what is possible.</p>
<p>To calculate the probability of an event, which here is defined as rolling a 1 on an unbiased die, we need to know two things: how many outcomes satisfy the criteria of our event (stated different, how many outcomes would count as what we are looking for) and the total number of outcomes possible. In our example, only a single outcome, rolling a 1, will satisfy our criteria, and there are a total of six possible outcomes (rolling a 1, rolling a 2, rolling a 3, rolling a 4, rolling a 5, and rolling a 6). Thus, the probability of rolling a 1 on an unbiased die is 1 in 6 or 1/6.</p>
<p>Put into an equation using generic terms, we get:</p>
<p>Probability = <span class="math inline">\(\frac{Number of Favorable  (desired) outcomes}{Total number of possible Outcomes}\)</span></p>
<p>We can also using P( ) as shorthand for probability and we can use A as shorthand for an event:</p>
<p>P(A) = <span class="math inline">\(\frac{Number of Favorable outcomes to A}{Total number of possible Outcomes}\)</span></p>
<p>Probability is usually symbolized by the letter p. The actual probability number is usually written as a decimal, though sometimes fractions or percentages are used. An event with a 50-50 chance is usually written as p = .5 but it could be written as p = 1/2 or p = 50%. It is also common to see probability written as being less than some value, using the less than (&lt;) sign. For example, p &lt; .05 means the probability of the event taking place is less than .05 or less than 5%.</p>
<p>Using the above equation, let’s calculate the probability of rolling an even number on this die:</p>
<blockquote>
<p>P(even number) = 2, 4, or 6/1, 2, 3, 4, 5, or 6 = 3/6 = .5. So we have a 50% chance of rolling an even number of this die.</p>
</blockquote>
<p>Let’s look at another example, let’s say that we are interested in knowing the probability of rain in Phoenix. We first have to define the activity — let’s say that we will look at the National Weather Service data for each day in 2020 and determine whether there was any rain at the downtown Phoenix weather station. According to these data, in 2020 there were 15 rainy days. To compute the probability of rain in Phoenix, we simply divide the number of rainy days by the number of days counted (365), giving p(rain in PHX in 2020) = 0.04.</p>
<p>Now that we have a probability formula, we can outline the formal features of probability (first defined by the Russian mathematician Andrei Kolmogorov). These are the features that a value has to have if it is going to be a probability.</p>
<ul>
<li>Probability cannot be negative.</li>
<li>The total probability of all outcomes in the sample space is 1;
<ul>
<li>that is, if we take the probability of each event and add them up, they must sum to 1.</li>
<li>This is interpreted as saying “Take all of the possible events and add up their probabilities. These must sum to one.”</li>
</ul></li>
<li>The probability of any individual event cannot be greater than one.<br />
</li>
<li>This is implied by the previous point; since they must sum to one, and they can’t be negative, then any particular probability cannot exceed one.</li>
</ul>
<p>To summarize, the probability that an event happens is the number of outcomes that qualify as that event (i.e. the number of ways the event could happen) compared to the total number of outcomes (i.e. how many things are possible). The principles laid out here operate under a certain set of conditions and can be elaborated into ideas that are complex yet powerful and elegant. However, such extensions are not necessary for a basic understanding of statistics, so we will end our discussion on the math of probability here. We will now return to a more familiar topic. This idea then brings us back around to our normal distribution, which can also be broken up into regions or areas, each of which are bounded by one or two z-scores and correspond to all z- scores in that region. The probability of randomly getting one of those z-scores in the specified region can then be found on a <strong>Standard Normal Distribution Table</strong>. Thus, the larger the region, the more likely an event is, and vice versa. Because the tails of the distribution are, by definition, smaller and we go farther out into the tail, the likelihood or probability of finding a result out in the extremes becomes small.</p>
</div>
</div>
<div id="probability-frequency-distributions" class="section level2 hasAnchor" number="3.2">
<h2><span class="header-section-number">3.2</span> Probability &amp; Frequency Distributions<a href="welcome-to-probability.html#probability-frequency-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>For our purposes, we will see shortly that the normal distribution is the key to how probability works. If you toss a fair coin four times, the outcomes may not be two heads and two tails. However, if you toss the same coin 4,000 times, the outcomes will be close to half heads and half tails. The expected theoretical probability of heads in any one toss is 1/2 or 0.5. Even though the outcomes of a few repetitions are uncertain, there is a regular pattern of outcomes when there are many repetitions (law of large numbers). The pattern tends to resemble a symmetrical normal distribution.</p>
<p>To help us think about probability, population, and inferential statistics we are going to use a frequency distribution because it can be seen as representing an entire population. This can be seen as a parallel concept because if all scores are represented in a frequency distribution it can function as a normal distribution. Using the empirical rule we know that different portions of the histogram can represent different proportions of the population and the terms proportions and probabilities mean the same thing. This means that a proportion of the histogram can correspond to the probability of a population.</p>
</div>
<div id="probability-in-normal-distributions" class="section level2 hasAnchor" number="3.3">
<h2><span class="header-section-number">3.3</span> Probability in Normal Distributions<a href="welcome-to-probability.html#probability-in-normal-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Recall that the normal distribution has an area under its curve that is equal to 1 and that it can be split into sections by drawing a line through it that corresponds to a given z-score. Because of this, we can interpret areas under the normal curve as probabilities that correspond to z-scores. In this section, we are going to link together the concepts of population, probability, and z-scores. We learned earlier that a frequency distribution can represent an entire population of scores. The shape of a frequency distribution for an entire population forms a symmetrical normal curve and certain proportions can be assigned to specific parts of the distribution.</p>
<div class="figure"><span style="display:block;" id="fig:stddiag"></span>
<img src="0601_probability_files/figure-html/stddiag-1.png" alt="Z-Score Distribution" width="672" />
<p class="caption">
Figure 3.1: Z-Score Distribution
</p>
</div>
<p>The graph above only shows us some of the proportions associated with specific z-score values but the Unit Normal Table lists all possible values for a normal distribution. This means that we can use z-scores to help us find the specific probability for a specific outcome or event.</p>
<p>First, let’s look back at the area between z = -1.00 and z = 1.00 presented in the next figure.</p>
<div class="figure"><span style="display:block;" id="fig:stddiag68"></span>
<img src="0601_probability_files/figure-html/stddiag68-1.png" alt="68% data come from the blue-shaded region" width="672" />
<p class="caption">
Figure 3.2: 68% data come from the blue-shaded region
</p>
</div>
<p>We were told earlier that this region contains 68% of the area under the curve. Thus, if we randomly chose a z-score from all possible z-scores, there is a 68% chance that it will be between z = -1.00 and z = 1.00 because those are the z-scores that satisfy our criteria.</p>
<p>Just like a pie chart is broken up into slices by drawing lines through it, we can also draw a line through the normal distribution to split it into sections. Take a look at the normal distribution in Figure 3 which has a line drawn through it as z = 1.25. This line creates two sections of the distribution: the smaller section called the tail and the larger section called the body. Differentiating between the body and the tail does not depend on which side of the distribution the line is drawn. All that matters is the relative size of the pieces: bigger is always body.</p>
<div class="figure"><span style="display:block;" id="fig:bodytail"></span>
<img src="img/bodytail.png" alt="Body and tail of the normal distribution" width="360" />
<p class="caption">
Figure 3.3: Body and tail of the normal distribution
</p>
</div>
<p>As you can see, we can break up the normal distribution into 3 pieces (lower tail, body, and upper tail) as in Figure 2 or into 2 pieces (body and tail) as in Figure 3. We can then find the proportion of the area in the body and tail based on where the line was drawn (i.e. at what z-score). Mathematically this is done using calculus.</p>
<p>Fortunately, the exact values are given you to you in a <em>Standard Normal Distribution Table</em>, also known at the <strong>z-table</strong>. Using the values in this table, we can find the area under the normal curve in any body, tail, or combination of tails no matter which z-scores are used to define them.</p>
<p>The z-table presents the values for the area under the curve to the left of the positive z-scores from 0.00-3.00 (technically 3.09), as indicated by the shaded region of the distribution at the top of the table. To find the appropriate value, we first find the row corresponding to our z-score then follow it over until we get to the column that corresponds to the number in the hundredths place of our z-score. For example, suppose we want to find the area in the body for a z-score of 1.62.</p>
<p>We would first find the row for 1.60 then follow it across to the column labeled</p>
<blockquote>
<p>(1.60 + 0.02 = 1.62) and find 0.9474 (see Figure 4). Thus, the odds of randomly selecting someone with a z-score less than (to the left of) z = 1.62 is 94.74% because that is the proportion of the area taken up by values that satisfy our criteria.</p>
</blockquote>
<div class="figure"><span style="display:block;" id="fig:normtable"></span>
<img src="img/normtable.jpeg" alt="Using the z-table to find the area in the body to the left of z = 1.62" width="501" />
<p class="caption">
Figure 3.4: Using the z-table to find the area in the body to the left of z = 1.62
</p>
</div>
<p>The z-table only presents the area in the body for positive z-scores because the normal distribution is symmetrical. Thus, the area in the body of z = 1.62 is equal to the area in the body for z = -1.62, though now the body will be the shaded area to the right of z (because the body is always larger). When in doubt, drawing out your distribution and shading the area you need to find will always help. The table also only presents the area in the body because the total area under the normal curve is always equal to 1.00, so if we need to find the area in the tail for z = 1.62, we simply find the area in the body and subtract it from 1.00 (1.00 – 0.9474 = 0.0526).</p>
<p>Let’s look at another example. This time, let’s find the area corresponding to z- scores more extreme than z = -1.96 and z = 1.96. That is, let’s find the area in the tails of the distribution for values less than z = -1.96 (farther negative and therefore more extreme) and greater than z = 1.96 (farther positive and therefore more extreme). This region is illustrated in Figure 5.</p>
<div class="figure"><span style="display:block;" id="fig:z196"></span>
<img src="0601_probability_files/figure-html/z196-1.png" alt="Area in the tails beyond z = -1.96 and z = 1.96" width="672" />
<p class="caption">
Figure 3.5: Area in the tails beyond z = -1.96 and z = 1.96
</p>
</div>
<p>Let’s start with the tail for z = 1.96. If we go to the z-table we will find that the body to the left of z = 1.96 is equal to 0.9750. To find the area in the tail, we subtract that from 1.00 to get 0.0250. Because the normal distribution is symmetrical, the area in the tail for z = -1.96 is the exact same value, 0.0250.</p>
<p>Finally, to get the total area in the shaded region, we simply add the areas together to get 0.0500. Thus, there is a 5% chance of randomly getting a value more extreme than z = -1.96 or z = 1.96 (this particular value and region will become incredibly important later on).</p>
<p>Finally, we can find the area between two z-scores by shading and subtracting. Figure 6 shows the area between z = 0.50 and z = 1.50. Because this is a subsection of a body (rather than just a body or a tail), we must first find the larger of the two bodies, in this case the body for z = 1.50, and subtract the smaller of the two bodies, or the body for z = 0.50. Aligning the distributions vertically, as in Figure 6, makes this clearer. From the z-table, the area in the body for z = 1.50 is 0.9332 and the area in the body for z = 0.50 is 0.6915. Subtracting these gives us 0.9332 – 0.6915 = 0.2417.</p>
<div class="figure"><span style="display:block;" id="fig:zclever"></span>
<img src="img/zclever.png" alt=" Area between z = 0.50 and 1.50, along with the corresponding areas in the body" width="420" />
<p class="caption">
Figure 3.6:  Area between z = 0.50 and 1.50, along with the corresponding areas in the body
</p>
</div>
</div>
<div id="considerations-in-understanding-probability-and-inferential-statistics-sampling" class="section level2 hasAnchor" number="3.4">
<h2><span class="header-section-number">3.4</span> Considerations in understanding probability and inferential statistics: sampling<a href="welcome-to-probability.html#considerations-in-understanding-probability-and-inferential-statistics-sampling" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Recall that the goal of inferential statistics is to draw conclusions or make predictions about large populations by using data from smaller samples that represent that population. Probability is the underlying concept of inferential statistics and forms a direct link between samples and the population that they come from.</p>
<div class="figure">
<img src="img/inf_probability.jpg" alt="The relationship between inferential statistics and probability" width="232" />
<p class="caption">
(#fig:inf_probability)The relationship between inferential statistics and probability
</p>
</div>
<p>As we learned earlier, gathering information about an entire population often costs too much or is virtually impossible. Instead, we use a sample of the population. A sample should have the same characteristics as the population it is representing. <strong>Random sampling</strong> is one method that may ensure representativeness of a sample. For our definition of probability to be consistent and accurate we must ensure two elements:</p>
<ol style="list-style-type: decimal">
<li>Every person in the population has an equal chance of being selected</li>
<li>Sampling occurs with replacement</li>
</ol>
<p>For <strong>random sampling</strong> a researcher starts with a <strong>complete list of the population</strong> (sometimes referred to as a sampling frame) and randomly selects some of them to an experiment. In this way every member of the population has an equal chance of being selected to participate. In order for the total number of outcomes to remain constant we need sampling with replacement – this means as one person is selected, another person must be added to keep the total number of possible outcomes the same. This is the full definition of random sampling.</p>
<p>For example, if the population is 25 people, the sample is ten, and you are sampling with replacement for any particular sample, then the chance of picking the first person is ten out of 25, and the chance of picking a different second person is nine out of 25 (you replace the first person). If you sample without replacement, then the chance of picking the first person is ten out of 25, and then the chance of picking the second person (who is different) is nine out of 24 (you do not replace the first person).</p>
<p>Compare the fractions to four decimal places:
- 9/25 = 0.3600
- 9/24 = 0.3750</p>
<p>It is clear that these numbers are not equivalent. Since we are using small samples as a stand-in for large populations in a research experiment, there will always be a certain level of uncertainty in our conclusions. How do we know that calculating statistical probability gives us the right number? The answer to this question comes from the <strong>law of large numbers</strong>, which shows that the empirical probability will approach the true probability as the sample size increases. We can see this by simulating a large number of coin flips, and looking at our estimate of the probability of heads after each flip.</p>
<p>The left panel of Figure 8 shows that as the number of samples (i.e., coin flip trials) increases, the estimated probability of heads converges onto the true value of 0.5. It’s unlikely that any of us has ever flipped a coin tens of thousands of times, but we are nonetheless willing to believe that the probability of flipping heads is 0.5. However, note that the estimates can be very far off from the true value when the sample sizes are small. A real-world example of this was seen in the 2017 special election for the US Senate in Alabama, which pitted the Republican Roy Moore against Democrat Doug Jones. The right panel of Figure 8 shows the relative amount of the vote reported for each of the candidates over the course of the evening, as an increasing number of ballots were counted. Early in the evening the vote counts were especially volatile, swinging from a large initial lead for Jones to a long period where Moore had the lead, until finally Jones took the lead to win the race.</p>
<div class="figure"><span style="display:block;" id="fig:lawlarge"></span>
<img src="img/inf_probability.jpg" alt="Left: A demonstration of the law of large numbers. A coin was flipped 30,000 times, and after each flip the probability of heads was computed based on the number of heads and tail collected up to that point. It takes about 15,000 flips for the probability to settle at the true probability of 0.5. Right: Relative proportion of the vote in the Dec 12, 2017 special election for the US Senate seat in Alabama, as a function of the percentage of precincts reporting. These data were transcribed from [here](https://www.ajc.com/news/national/alabama-senate-race-live-updates-roy-moore-doug-jones/KPRfkdaweoiXICW3FHjXqI/)" width="232" />
<p class="caption">
Figure 3.7: Left: A demonstration of the law of large numbers. A coin was flipped 30,000 times, and after each flip the probability of heads was computed based on the number of heads and tail collected up to that point. It takes about 15,000 flips for the probability to settle at the true probability of 0.5. Right: Relative proportion of the vote in the Dec 12, 2017 special election for the US Senate seat in Alabama, as a function of the percentage of precincts reporting. These data were transcribed from <a href="https://www.ajc.com/news/national/alabama-senate-race-live-updates-roy-moore-doug-jones/KPRfkdaweoiXICW3FHjXqI/">here</a>
</p>
</div>
<p>These two examples show that while large samples will ultimately converge on the true probability, the results with small samples can be far off. Unfortunately, many people forget this and overinterpret results from small samples. This was referred to as the <em>law of small numbers</em> by the psychologists Danny Kahneman and Amos Tversky, who showed that people (even trained researchers) often behave as if the law of large numbers applies even to small samples, giving too much credence to results based on small datasets. We will see examples throughout the course of just how unstable statistical results can be when they are generated on the basis of small samples. Furthermore, a very important point: even if we are able to make an accurate prediction, we can never prove with 100% certainty that this prediction will hold true in all possible situations. This is because samples never have exactly the same characteristics of the population from which they come from. Therefore, we will always have some level of uncertainty about whether the results found in our sample are also found in the population. The best we can do is infer what is most likely to be found. Our level of confidence in an inferential statistic or the outcome of an experiment is represented through probability theory.</p>
</div>
<div id="more-probability-terms" class="section level2 hasAnchor" number="3.5">
<h2><span class="header-section-number">3.5</span> More Probability Terms<a href="welcome-to-probability.html#more-probability-terms" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A <strong>probability distribution</strong> describes the probability of all of the possible outcomes in an activity. For example, on Jan 20 2018, the basketball player Steph Curry hit only 2 out of 4 free throws in a game against the Houston Rockets. We know that Curry’s overall probability of hitting free throws across the entire season was 0.91, so it seems pretty unlikely that he would hit only 50% of his free throws in a game, but exactly how unlikely is it? We can determine this using a theoretical probability distribution; throughout this book we will encounter a number of these probability distributions, each of which is appropriate to describe different types of data. In this case, we use the binomial distribution, which provides a way to compute the probability of some number of successes out of a number of trials on which there is either success or failure and nothing in between (only 2 outcomes), given some known probability of success on each trial. In this example, if we calculated the probability of Curry making 2 of 4 free throws when his season average was .91, it would come out to a 4% chance<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>. This shows that given Curry’s overall free throw percentage, it is very unlikely that he would hit only 2 out of 4 free throws. This just goes to show that unlikely things do actually happen in the real world.</p>
<p>Often though, we might want to know how likely it is to find a value that is as extreme or more than a particular value versus a specific value; this will become very important when we discuss hypothesis testing later. To answer this question, we can use a <strong>cumulative probability distribution</strong>; whereas a standard probability distribution tells us the probability of some specific value, the cumulative distribution tells us the probability of a value as large or larger (or as small or smaller) than some specific value. For the Curry free-throw example, this would be .043<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a>. In many cases the number of possible outcomes would be too large for us to compute the cumulative probability by enumerating all possible values; fortunately, it can be computed directly for any theoretical probability distribution.</p>
<p>Although we have information, sometimes we can be mislead. Let’s use the example of a coin toss. Each toss is independent of one another with only 2 outcomes {heads, tails}. What if I want to calculate the probability of getting heads for a coin toss? I might base that on my previous 12 coin flips. A coin might come up heads 8 times out of 12 flips, for a relative frequency of 8/12 or 2/3. Again, the probability is usually calculated as the proportion of successful outcomes divided by the number of all possible outcomes. What happens if I toss the coin again? What is the probability that it will be heads again? Do I use information from my previous 12 flips? Well, the answer is ½ or 50%. That is because in our scenario, each toss is independent and each outcome is independent. This means that the outcome of the ninth toss is not related to the previous toss. To assume otherwise is committing what we call <strong>gambler’s fallacy</strong>.</p>
<div id="statistical-independence" class="section level3 hasAnchor" number="3.5.1">
<h3><span class="header-section-number">3.5.1</span> Statistical Independence<a href="welcome-to-probability.html#statistical-independence" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The term “independent” has a very specific meaning in statistics, which is somewhat different from the common usage of the term. <strong>Statistical independence</strong> between two variables means that knowing the value of one variable doesn’t tell us anything about the value of the other. This can be expressed as: P(A|B)=P(A). That is, the probability of A given some value of B is just the same as the overall probability of A (because they are independent). Again, while independence in common language often refers to sets that are exclusive, statistical independence refers to the case where one cannot predict anything about one variable from the value of another variable. For example, knowing a person’s hair color is unlikely to tell you whether they prefer chocolate or strawberry ice cream. Later in the book we will discuss statistical tools that will let us directly test whether two variables are independent.</p>
<p>Sometimes we want to quantify the relation between probabilities more directly, which we can do by converting them into odds which express the relative likelihood of something happening or not. We can use odds to compare different probabilities, by computing what is called an <strong>odds ratio</strong> – which is exactly what it sounds like. For example, let’s say that we want to know how much the positive test increases the individual’s odds of having cancer. We can first compute the prior odds – that is, the odds before we knew that the person had tested positively. An odds ratio is an example of what we will later call an effect size, which is a way of quantifying how relatively large any particular statistical effect is.</p>
<p>A final point relates to how probabilities have been interpreted. Historically, there have been two different ways that probabilities have been interpreted. The first (known as the frequentist interpretation) interprets probabilities in terms of long-run frequencies. For example, in the case of a coin flip, it would reflect the relative frequencies of heads in the long run after a large number of flips. While this interpretation might make sense for events that can be repeated many times like a coin flip, it makes less sense for events that will only happen once, like an individual person’s life or a particular presidential election; and as the economist John Maynard Keynes famously said, “In the long run, we are all dead.” The other interpretation of probabilities (known as the Bayesian interpretation) is as a degree of belief in a particular proposition. If I were to ask you “How likely is it that the US will return to the moon by 2040”, you can provide an answer to this question based on your knowledge and beliefs, even though there are no relevant frequencies to compute a frequentist probability. One way that we often frame subjective probabilities is in terms of one’s willingness to accept a particular gamble. For example, if you think that the probability of the US landing on the moon by 2040 is 0.1 (i.e. odds of 9 to 1), then that means that you should be willing to accept a gamble that would pay off with anything more than 9 to 1 odds if the event occurs. As we will see, these two different definitions of probability are very relevant to the two different ways that statisticians think about testing statistical hypotheses, which we will encounter in later chapters.</p>
</div>
</div>
<div id="recap" class="section level2 hasAnchor" number="3.6">
<h2><span class="header-section-number">3.6</span> Recap<a href="welcome-to-probability.html#recap" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Probability is a mathematical tool used to study randomness. It deals with the chance (the likelihood) of an event occurring. The theory of probability began with the study of games of chance such as poker. Predictions take the form of probabilities. To predict the likelihood of an earthquake, of rain, or whether you will get an A in this course, we use probabilities. Doctors use probability to determine the chance of a vaccination causing the disease the vaccination is supposed to prevent. A stockbroker uses probability to determine the rate of return on a client’s investments. You might use probability to decide to buy a lottery ticket or not. In your study of statistics, you will use the power of mathematics through probability calculations to analyze and interpret your data.</p>
<div id="learning-objectives" class="section level3 hasAnchor" number="3.6.1">
<h3><span class="header-section-number">3.6.1</span> Learning objectives<a href="welcome-to-probability.html#learning-objectives" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Having read this chapter, you should be able to:</p>
<p>Describe the sample space for a selected random experiment.
Describe the law of large numbers.
Describe the difference between a probability and a conditional probability
Describe the relationship between z-scores and the standard unit normal table (z-table)
Probability is a tough topic for everyone, but the tools it gives us are incredibly powerful and enable us to do amazing things with data analysis. They are the heart of how inferential statistics work.</p>
</div>
<div id="exercises" class="section level3 hasAnchor" number="3.6.2">
<h3><span class="header-section-number">3.6.2</span> Exercises<a href="welcome-to-probability.html#exercises" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ol style="list-style-type: decimal">
<li>In your own words, what is probability?</li>
<li>There is a bag with 5 red blocks, 2 yellow blocks, and 4 blue blocks. If you reach in and grab one block without looking, what is the probability it is red?</li>
<li>Under a normal distribution, which of the following is more likely? (Note: this question can be answered without any calculations if you draw out the distributions and shade properly)
<ul>
<li>Getting a z-score greater than z = 2.75</li>
<li>Getting a z-score less than z = -1.50</li>
</ul></li>
<li>The heights of women in the United States are normally distributed with a mean of 63.7 inches and a standard deviation of 2.7 inches. If you randomly select a woman in the United States, what is the probability that she will be between 65 and 67 inches tall?</li>
<li>The heights of men in the United States are normally distributed with a mean of 69.1 inches and a standard deviation of 2.9 inches. 6. What proportion of men are taller than 6 feet (72 inches)?</li>
<li>You know you need to score at least 82 points on the final exam to pass your class. After the final, you find out that the average score on the exam was 78 with a standard deviation of 7. How likely is it that you pass the class?</li>
<li>What proportion of the area under the normal curve is greater than z = 1.65?</li>
<li>Find the z-score that bounds 25% of the lower tail of the distribution.</li>
<li>Find the z-score that bounds the top 9% of the distribution.</li>
<li>In a distribution with a mean of 70 and standard deviation of 12, what proportion of scores are lower than 55?</li>
</ol>

</div>
</div>
</div>



<!-- keep-->
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>The fine print calculating Steph Curry’s free throws probability: P(2;4,0.91)=(42)0.912(1−0.91)4−2=0.040 <a href="welcome-to-probability.html#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>Fine print: To determine this, we could use the binomial probability equation and plug in all of the possible values of outcomes and add them together: P(k≤2)=P(k=2)+P(k=1)+P(k=0)=6e−5+.002+.040=.043 P(k)= P(k=2) + P(k=1) + P(k=0) = 6e^{-5} + .002 + .040 = .043<a href="welcome-to-probability.html#fnref2" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="welcome-to-univariate-statistics.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="probability.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": true,
    "facebook": false,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": "https://github.com/psychmethods/coursenotes/edit/main/0601_probability.Rmd",
    "text": "Edit"
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": ["https://github.com/psychmethods/coursenotes/raw/main/0601_probability.Rmd", "CN4P.epub"],
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "section"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
