[["index.html", "Psychology Research Methods For Psych Minors Welcome to PSY 310", " Psychology Research Methods For Psych Minors S. Mason Garrison 2022-02-13 Welcome to PSY 310 Welcome to class! This website is designed to accompany Mason Garrisons Psych Methods for Psych Minors (PMPM). This class is a undergraduate-level psychology course at Wake Forest University. I encourage anyone who is methods-curious to work their way through these course notes. The course notes include lectures, worked examples, readings, activities, and labs. You can find the current version of the course syllabus here, along with all of the other syllabi for my classes. All the embedded lecture videos can be found on a youtube playlist. "],["attribution.html", "Attribution Major Attributions Additional Attributions", " Attribution This class leans heavily on other peoples materials and ideas. I have done my best to document the origin of the materials and ideas. In particular, I have noted those people whose work has been a major contribution as well as those who have additional contributions. You can see specific changes by examining the [edit history on the git repo][edits] Major Attributions Jenny Bryans (jennybryan) STAT 545 Joe Rodgerss 2101 Alisa Beyer Introduction to Statistics for Psychology Danielle Navarros Learning Statistics with R Additional Attributions Eric Stones 210 Eranda Jayawickremes 310 "],["license.html", "License", " License This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License. This information is a human-readable summary of (and not a substitute for) the license. Please see https://creativecommons.org/licenses/by-sa/4.0/legalcode for the full legal text. You are free to: Sharecopy and redistribute the material in any medium or format Remixremix, transform, and build upon the material for any purpose, even commercially. The licensor cannot revoke these freedoms as long as you follow the license terms. Under the following terms: AttributionYou must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use. ShareAlikeIf you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original. No additional restrictionsYou may not apply legal terms or technological measures that legally restrict others from doing anything the license permits. Notices: You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation. No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material. "],["colophon.html", "Colophon", " Colophon These notes was written in bookdown inside RStudio. The [website][web] is hosted with github, The complete source is available from [github][git]. The book style is based on a design by Desirée De Leon. This version of the notes was built with: #&gt; Finding R package dependencies ... Done! #&gt; setting value #&gt; version R version 4.0.5 (2021-03-31) #&gt; os Windows 10 x64 (build 19044) #&gt; system x86_64, mingw32 #&gt; ui RTerm #&gt; language (EN) #&gt; collate English_United States.1252 #&gt; ctype English_United States.1252 #&gt; tz America/New_York #&gt; date 2022-02-13 #&gt; pandoc 2.11.4 @ C:/Program Files/RStudio/bin/pandoc/ (via rmarkdown) Along with these packages: "],["dont-miss-module-00.html", "Dont Miss Module 00 0.1 Learning Goals for this Module (Chapter 0) 0.2 To-Do List 0.3 Course Modality 0.4 Knowledge is Power 0.5 Meet Mason 0.6 Website Tour", " Dont Miss Module 00 This overview is designed to orient you to the class as well as show you the rationale underlying them. Every module will follow a similar structure. First, Ill provide an overview of the topic and provide actionable learning goals. Please watch the videos from this playlist and work your way through the notes. Although the module-level playlists are embedded in the course, you can find the full-course video playlist here. By the end of each module, you should be able to achieve all the learning goals outlined below. For this module those goals are 0.1 Learning Goals for this Module (Chapter 0) Explain the structure of the course. Understand the rationale underlying the structure of the course. Explain the expectations for your section. 0.2 To-Do List I provide a list of tasks youll need to do to complete the module. I provide this list to give the class structure. These tasks are broken down into three stages: explore, engage, and evaluate These stages are designed to be sequential. Complete the explore tasks first. Second, complete the engage tasks. Last, complete the evaluation. For each stage, I will explicitly indicate whether specific tasks are required or merely, recommended. The exploring portion of the class introduces you to the content of the module. It will consist of reading texts and viewing videos. Relative to the other two portions of the module, its more passive. The engagement portion of the class facilitates you engaging with the material, interacting with the other members of the learning community. Students will complete two engagement activities. Most of the time, Ill provide multiple choices for the engagement portion of the module. In order to successfully complete Module 0, please do the following: Explore: Skim: the preface of your textbook (Optional) View: the embedded videos connected to the learning goals. Engage: Create: a brief video introducing yourself and post it on the discussion board. Complete: the following survey. Evaluate: 0.3 Course Modality This class is a flipped class. The online portions are asynchronous, and often contain pre-recorded videos. Ive created a video highlighting how to be a successful asynchronous learner. Much of this information comes from Northeastern Universitys Tips for Taking Online Classes 0.3.1 Productivity During Lockdown 0.4 Knowledge is Power This brief video is covers the icebreaker I do in all of my classes. I encourage you to watch it. In it, I discuss stereotype threats and statistics anxiety. 0.5 Meet Mason Mason Garrison is an assistant professor of quantitative psychology at Wake Forest University. She earned her PhD and MS in Quantitative Methods from Vanderbilt University, and her AB in Economics &amp; Psychology from Washington University in St. Louis. Her research is an eclectic mix of behavior genetics, individual differences (e.g., personality, intelligence), and statistics. On a more humanizing note, at the start of quarantine, Mason was fostering two feral kittens. They had been living behind the freshman dorms. She made the mistake of naming them. So now she has three cats Tukey (namesake: John Tukey) Archie (namesake: Archimedes) Annie (namesake: Anne Anastasi) 0.6 Website Tour "],["guidance.html", "Guidance 0.7 Syllabus 0.8 Materials", " Guidance 0.7 Syllabus This document is not the syllabus. Please refer to Mason Garrisons syllabus here: https://smasongarrison.github.io/syllabi/ 0.8 Materials 0.8.1 Hardware This class requires that you have a laptop that can run R. 0.8.2 Required Texts The text is intended to supplement the videos, lecture notes, tutorials, activities, and labs. You probably need to consume all of them in order to be successful in this class. 0.8.3 Software 0.8.3.1 R and RStudio R is a free software environment for statistical computing and graphics. It compiles and runs on a wide variety of UNIX platforms, Windows, and MacOS. RStudio is a free integrated development environment (IDE), a powerful user interface for R. 0.8.3.2 Git and Github Git is a version control system. Its original purpose was to help groups of developers work collaboratively on big software projects. Git manages the evolution of a set of files  called a repository  in a structured way. Think of it like the Track Changes features from Microsoft Word. Github is a free IDE and hosting service for Git. As a Wake Forest student, you should be able to access the GitHub Student Developer Pack for free. It includes a free PRO upgrade for your github account. "],["welcome-to-probability.html", "1 Welcome to Probability 1.1 What is probability? 1.2 Probability &amp; Frequency Distributions 1.3 Probability in Normal Distributions 1.4 Considerations in understanding probability and inferential statistics: sampling 1.5 More Probability Terms", " 1 Welcome to Probability In this lesson, we start to move away from descriptive statistics and begin our transition into inferential statistics. Recall that the goal of inferential statistics is to draw conclusions or make predictions about large populations by using data from smaller samples that represent that population. Probability is the underlying concept of inferential statistics and forms a direct link between samples and the population that they come from. In this chapter we will focus only on the principles and ideas necessary to lay the groundwork for future inferential statistics. We accomplish this by quickly tying the concepts of probability to what we already know about normal distributions and z-scores. 1.1 What is probability? Informally, we usually think of probability as a number that describes the likelihood of some event occurring, which ranges from zero (impossibility) to one (certainty). Probability can be discussed more vaguely. Chances are low it will rain today. Given there are no clouds, chances are low it will rain today. Given is the word we use to state what the conditions are. As the conditions change, so does the probability. Thus, if it were cloudy and windy outside, we might say, given the current weather conditions, there is a high probability that it is going to rain. It should also be noted that the terms low and high are relative and vague, and they will likely be interpreted different by different people (in other words: given how vague the terminology was, the probability of different interpretations is high). In statistics, most of the time we try to use more precise language or, even better, numbers to represent the probability of our event. Regardless, the basic structure and logic of our statements are consistent with how we speak about probability using numbers and formulas. Sometimes probabilities will instead be expressed in percentages, which range from zero to one hundred, as when the weather forecast predicts a twenty percent chance of rain today. In each case, these numbers are expressing how likely that particular event is, ranging from absolutely impossible (0%) to absolutely certain (100%). When we speak of the probability of something happening, we are talking how likely it is that thing will happen based on the conditions present. To formalize probability theory, we first need to define a few terms: Probability theory is the branch of mathematics that deals with chance and uncertainty. It forms an important part of the foundation for statistics, because it provides us with the mathematical tools to describe uncertain events. The study of probability arose in part due to interest in understanding games of chance, like cards or dice. These games provide useful examples of many statistical concepts, because when we repeat these games the likelihood of different outcomes remains (mostly) the same. To calculate probability, we need an activity that produces or observes an outcome is needed. This is typically an experiment or any situation or activity in which the result is not known in advance. Examples are the outcome for: the weather outside, flipping a coin, rolling a 6-sided die, or trying a new route to work to see if its faster than the old route. We also need to know the known outcomes of the activity/event/experiment. The sample space is the set of possible outcomes for an activity. We represent these by listing them within a set of squiggly brackets. For rain is {rain, not rain} For a coin flip, the sample space is {heads, tails}. For a six-sided die, the sample space is each of the possible numbers that can appear: {1,2,3,4,5,6}. For the amount of time it takes to get to work, the sample space is all possible real numbers greater than zero (since it cant take a negative amount of time to get somewhere, at least not yet). We wont bother trying to write out all of those numbers within the brackets. An outcome or event is a subset of the sample space to examine specific probability. In principle, it could be one or more of possible outcomes in the sample space, but here we will focus primarily on elementary events which consist of exactly one possible outcome. An event is a catch-all term to talk about any specific thing happening. For example, this could be it rains, obtaining heads in a single coin flip, rolling a 4 on a throw of the die, or taking 21 minutes to get home by the new route. In statistics, we usually define probability as the expected relative frequency of a particular outcome. The relative frequency is the number of times an event takes place relative to the number of times it could have taken place. Lets look at a slightly deeper example before we learn a basic probability formula. Say we have a regular, six-sided die (note that die is singular and dice is plural, a distinction that can be hard to get correct on the first try) and want to know how likely it is that we will roll a 1. That is, what is the probability of rolling a 1, given that the die is not weighted (which would introduce what we call a bias, though that is beyond the scope of this chapter). We could roll the die and see if it is a 1 or not, but that wont tell us about the probability, it will only tell us a single result. We could also roll the die hundreds or thousands of times, recording each outcome and seeing what the final list looks like, but this is time-consuming, and rolling a die that many times may lead down a dark path to gambling or, worse, playing Dungeons &amp; Dragons. What we need is a simple equation that represents what we are looking for and what is possible. To calculate the probability of an event, which here is defined as rolling a 1 on an unbiased die, we need to know two things: how many outcomes satisfy the criteria of our event (stated different, how many outcomes would count as what we are looking for) and the total number of outcomes possible. In our example, only a single outcome, rolling a 1, will satisfy our criteria, and there are a total of six possible outcomes (rolling a 1, rolling a 2, rolling a 3, rolling a 4, rolling a 5, and rolling a 6). Thus, the probability of rolling a 1 on an unbiased die is 1 in 6 or 1/6. Put into an equation using generic terms, we get: Probability = \\(\\frac{Number of Favorable (desired) outcomes}{Total number of possible Outcomes}\\) We can also using P( ) as shorthand for probability and we can use A as shorthand for an event: P(A) = \\(\\frac{Number of Favorable outcomes to A}{Total number of possible Outcomes}\\) Probability is usually symbolized by the letter p. The actual probability number is usually written as a decimal, though sometimes fractions or percentages are used. An event with a 50-50 chance is usually written as p = .5 but it could be written as p = 1/2 or p = 50%. It is also common to see probability written as being less than some value, using the less than (&lt;) sign. For example, p &lt; .05 means the probability of the event taking place is less than .05 or less than 5%. Using the above equation, lets calculate the probability of rolling an even number on this die: P(even number) = 2, 4, or 6/1, 2, 3, 4, 5, or 6 = 3/6 = .5. So we have a 50% chance of rolling an even number of this die. Lets look at another example, lets say that we are interested in knowing the probability of rain in Phoenix. We first have to define the activity  lets say that we will look at the National Weather Service data for each day in 2020 and determine whether there was any rain at the downtown Phoenix weather station. According to these data, in 2020 there were 15 rainy days. To compute the probability of rain in Phoenix, we simply divide the number of rainy days by the number of days counted (365), giving p(rain in PHX in 2020) = 0.04. Now that we have a probability formula, we can outline the formal features of probability (first defined by the Russian mathematician Andrei Kolmogorov). These are the features that a value has to have if it is going to be a probability. Probability cannot be negative. The total probability of all outcomes in the sample space is 1; that is, if we take the probability of each event and add them up, they must sum to 1. This is interpreted as saying Take all of the possible events and add up their probabilities. These must sum to one. The probability of any individual event cannot be greater than one. - This is implied by the previous point; since they must sum to one, and they cant be negative, then any particular probability cannot exceed one. To summarize, the probability that an event happens is the number of outcomes that qualify as that event (i.e. the number of ways the event could happen) compared to the total number of outcomes (i.e. how many things are possible). The principles laid out here operate under a certain set of conditions and can be elaborated into ideas that are complex yet powerful and elegant. However, such extensions are not necessary for a basic understanding of statistics, so we will end our discussion on the math of probability here. We will now return to a more familiar topic. This idea then brings us back around to our normal distribution, which can also be broken up into regions or areas, each of which are bounded by one or two z-scores and correspond to all z- scores in that region. The probability of randomly getting one of those z-scores in the specified region can then be found on a Standard Normal Distribution Table. Thus, the larger the region, the more likely an event is, and vice versa. Because the tails of the distribution are, by definition, smaller and we go farther out into the tail, the likelihood or probability of finding a result out in the extremes becomes small. 1.2 Probability &amp; Frequency Distributions For our purposes, we will see shortly that the normal distribution is the key to how probability works. If you toss a fair coin four times, the outcomes may not be two heads and two tails. However, if you toss the same coin 4,000 times, the outcomes will be close to half heads and half tails. The expected theoretical probability of heads in any one toss is 1/2 or 0.5. Even though the outcomes of a few repetitions are uncertain, there is a regular pattern of outcomes when there are many repetitions (law of large numbers). The pattern tends to resemble a symmetrical normal distribution. To help us think about probability, population, and inferential statistics we are going to use a frequency distribution because it can be seen as representing an entire population. This can be seen as a parallel concept because if all scores are represented in a frequency distribution it can function as a normal distribution. Using the empirical rule we know that different portions of the histogram can represent different proportions of the population and the terms proportions and probabilities mean the same thing. This means that a proportion of the histogram can correspond to the probability of a population. 1.3 Probability in Normal Distributions Recall that the normal distribution has an area under its curve that is equal to 1 and that it can be split into sections by drawing a line through it that corresponds to a given z-score. Because of this, we can interpret areas under the normal curve as probabilities that correspond to z-scores. In this section, we are going to link together the concepts of population, probability, and z-scores. We learned earlier that a frequency distribution can represent an entire population of scores. The shape of a frequency distribution for an entire population forms a symmetrical normal curve and certain proportions can be assigned to specific parts of the distribution. Figure 1.1: Z-Score Distribution The graph above only shows us some of the proportions associated with specific z-score values but the Unit Normal Table lists all possible values for a normal distribution. This means that we can use z-scores to help us find the specific probability for a specific outcome or event. First, lets look back at the area between z = -1.00 and z = 1.00 presented in the next figure. Figure 1.2: 68% data come from the blue-shaded region We were told earlier that this region contains 68% of the area under the curve. Thus, if we randomly chose a z-score from all possible z-scores, there is a 68% chance that it will be between z = -1.00 and z = 1.00 because those are the z-scores that satisfy our criteria. Just like a pie chart is broken up into slices by drawing lines through it, we can also draw a line through the normal distribution to split it into sections. Take a look at the normal distribution in Figure 3 which has a line drawn through it as z = 1.25. This line creates two sections of the distribution: the smaller section called the tail and the larger section called the body. Differentiating between the body and the tail does not depend on which side of the distribution the line is drawn. All that matters is the relative size of the pieces: bigger is always body. Figure 1.3: Body and tail of the normal distribution As you can see, we can break up the normal distribution into 3 pieces (lower tail, body, and upper tail) as in Figure 2 or into 2 pieces (body and tail) as in Figure 3. We can then find the proportion of the area in the body and tail based on where the line was drawn (i.e. at what z-score). Mathematically this is done using calculus. Fortunately, the exact values are given you to you in a Standard Normal Distribution Table, also known at the z-table. Using the values in this table, we can find the area under the normal curve in any body, tail, or combination of tails no matter which z-scores are used to define them. The z-table presents the values for the area under the curve to the left of the positive z-scores from 0.00-3.00 (technically 3.09), as indicated by the shaded region of the distribution at the top of the table. To find the appropriate value, we first find the row corresponding to our z-score then follow it over until we get to the column that corresponds to the number in the hundredths place of our z-score. For example, suppose we want to find the area in the body for a z-score of 1.62. We would first find the row for 1.60 then follow it across to the column labeled (1.60 + 0.02 = 1.62) and find 0.9474 (see Figure 4). Thus, the odds of randomly selecting someone with a z-score less than (to the left of) z = 1.62 is 94.74% because that is the proportion of the area taken up by values that satisfy our criteria. Figure 1.4: Using the z-table to find the area in the body to the left of z = 1.62 The z-table only presents the area in the body for positive z-scores because the normal distribution is symmetrical. Thus, the area in the body of z = 1.62 is equal to the area in the body for z = -1.62, though now the body will be the shaded area to the right of z (because the body is always larger). When in doubt, drawing out your distribution and shading the area you need to find will always help. The table also only presents the area in the body because the total area under the normal curve is always equal to 1.00, so if we need to find the area in the tail for z = 1.62, we simply find the area in the body and subtract it from 1.00 (1.00  0.9474 = 0.0526). Lets look at another example. This time, lets find the area corresponding to z- scores more extreme than z = -1.96 and z = 1.96. That is, lets find the area in the tails of the distribution for values less than z = -1.96 (farther negative and therefore more extreme) and greater than z = 1.96 (farther positive and therefore more extreme). This region is illustrated in Figure 5. Figure 1.5: Area in the tails beyond z = -1.96 and z = 1.96 Lets start with the tail for z = 1.96. If we go to the z-table we will find that the body to the left of z = 1.96 is equal to 0.9750. To find the area in the tail, we subtract that from 1.00 to get 0.0250. Because the normal distribution is symmetrical, the area in the tail for z = -1.96 is the exact same value, 0.0250. Finally, to get the total area in the shaded region, we simply add the areas together to get 0.0500. Thus, there is a 5% chance of randomly getting a value more extreme than z = -1.96 or z = 1.96 (this particular value and region will become incredibly important later on). Finally, we can find the area between two z-scores by shading and subtracting. Figure 6 shows the area between z = 0.50 and z = 1.50. Because this is a subsection of a body (rather than just a body or a tail), we must first find the larger of the two bodies, in this case the body for z = 1.50, and subtract the smaller of the two bodies, or the body for z = 0.50. Aligning the distributions vertically, as in Figure 6, makes this clearer. From the z-table, the area in the body for z = 1.50 is 0.9332 and the area in the body for z = 0.50 is 0.6915. Subtracting these gives us 0.9332  0.6915 = 0.2417. Figure 1.6: Area between z = 0.50 and 1.50, along with the corresponding areas in the body 1.4 Considerations in understanding probability and inferential statistics: sampling Recall that the goal of inferential statistics is to draw conclusions or make predictions about large populations by using data from smaller samples that represent that population. Probability is the underlying concept of inferential statistics and forms a direct link between samples and the population that they come from. (#fig:inf_probability)The relationship between inferential statistics and probability As we learned earlier, gathering information about an entire population often costs too much or is virtually impossible. Instead, we use a sample of the population. A sample should have the same characteristics as the population it is representing. Random sampling is one method that may ensure representativeness of a sample. For our definition of probability to be consistent and accurate we must ensure two elements: Every person in the population has an equal chance of being selected Sampling occurs with replacement For random sampling a researcher starts with a complete list of the population (sometimes referred to as a sampling frame) and randomly selects some of them to an experiment. In this way every member of the population has an equal chance of being selected to participate. In order for the total number of outcomes to remain constant we need sampling with replacement  this means as one person is selected, another person must be added to keep the total number of possible outcomes the same. This is the full definition of random sampling. For example, if the population is 25 people, the sample is ten, and you are sampling with replacement for any particular sample, then the chance of picking the first person is ten out of 25, and the chance of picking a different second person is nine out of 25 (you replace the first person). If you sample without replacement, then the chance of picking the first person is ten out of 25, and then the chance of picking the second person (who is different) is nine out of 24 (you do not replace the first person). Compare the fractions to four decimal places: - 9/25 = 0.3600 - 9/24 = 0.3750 It is clear that these numbers are not equivalent. Since we are using small samples as a stand-in for large populations in a research experiment, there will always be a certain level of uncertainty in our conclusions. How do we know that calculating statistical probability gives us the right number? The answer to this question comes from the law of large numbers, which shows that the empirical probability will approach the true probability as the sample size increases. We can see this by simulating a large number of coin flips, and looking at our estimate of the probability of heads after each flip. The left panel of Figure 8 shows that as the number of samples (i.e., coin flip trials) increases, the estimated probability of heads converges onto the true value of 0.5. Its unlikely that any of us has ever flipped a coin tens of thousands of times, but we are nonetheless willing to believe that the probability of flipping heads is 0.5. However, note that the estimates can be very far off from the true value when the sample sizes are small. A real-world example of this was seen in the 2017 special election for the US Senate in Alabama, which pitted the Republican Roy Moore against Democrat Doug Jones. The right panel of Figure 8 shows the relative amount of the vote reported for each of the candidates over the course of the evening, as an increasing number of ballots were counted. Early in the evening the vote counts were especially volatile, swinging from a large initial lead for Jones to a long period where Moore had the lead, until finally Jones took the lead to win the race. Figure 1.7: Left: A demonstration of the law of large numbers. A coin was flipped 30,000 times, and after each flip the probability of heads was computed based on the number of heads and tail collected up to that point. It takes about 15,000 flips for the probability to settle at the true probability of 0.5. Right: Relative proportion of the vote in the Dec 12, 2017 special election for the US Senate seat in Alabama, as a function of the percentage of precincts reporting. These data were transcribed from here These two examples show that while large samples will ultimately converge on the true probability, the results with small samples can be far off. Unfortunately, many people forget this and overinterpret results from small samples. This was referred to as the law of small numbers by the psychologists Danny Kahneman and Amos Tversky, who showed that people (even trained researchers) often behave as if the law of large numbers applies even to small samples, giving too much credence to results based on small datasets. We will see examples throughout the course of just how unstable statistical results can be when they are generated on the basis of small samples. Furthermore, a very important point: even if we are able to make an accurate prediction, we can never prove with 100% certainty that this prediction will hold true in all possible situations. This is because samples never have exactly the same characteristics of the population from which they come from. Therefore, we will always have some level of uncertainty about whether the results found in our sample are also found in the population. The best we can do is infer what is most likely to be found. Our level of confidence in an inferential statistic or the outcome of an experiment is represented through probability theory. 1.5 More Probability Terms A probability distribution describes the probability of all of the possible outcomes in an activity. For example, on Jan 20 2018, the basketball player Steph Curry hit only 2 out of 4 free throws in a game against the Houston Rockets. We know that Currys overall probability of hitting free throws across the entire season was 0.91, so it seems pretty unlikely that he would hit only 50% of his free throws in a game, but exactly how unlikely is it? We can determine this using a theoretical probability distribution; throughout this book we will encounter a number of these probability distributions, each of which is appropriate to describe different types of data. In this case, we use the binomial distribution, which provides a way to compute the probability of some number of successes out of a number of trials on which there is either success or failure and nothing in between (only 2 outcomes), given some known probability of success on each trial. In this example, if we calculated the probability of Curry making 2 of 4 free throws when his season average was .91, it would come out to a 4% chance1. This shows that given Currys overall free throw percentage, it is very unlikely that he would hit only 2 out of 4 free throws. This just goes to show that unlikely things do actually happen in the real world. Often though, we might want to know how likely it is to find a value that is as extreme or more than a particular value versus a specific value; this will become very important when we discuss hypothesis testing later. To answer this question, we can use a cumulative probability distribution; whereas a standard probability distribution tells us the probability of some specific value, the cumulative distribution tells us the probability of a value as large or larger (or as small or smaller) than some specific value. For the Curry free-throw example, this would be .0432. In many cases the number of possible outcomes would be too large for us to compute the cumulative probability by enumerating all possible values; fortunately, it can be computed directly for any theoretical probability distribution. Although we have information, sometimes we can be mislead. Lets use the example of a coin toss. Each toss is independent of one another with only 2 outcomes {heads, tails}. What if I want to calculate the probability of getting heads for a coin toss? I might base that on my previous 12 coin flips. A coin might come up heads 8 times out of 12 flips, for a relative frequency of 8/12 or 2/3. Again, the probability is usually calculated as the proportion of successful outcomes divided by the number of all possible outcomes. What happens if I toss the coin again? What is the probability that it will be heads again? Do I use information from my previous 12 flips? Well, the answer is ½ or 50%. That is because in our scenario, each toss is independent and each outcome is independent. This means that the outcome of the ninth toss is not related to the previous toss. To assume otherwise is committing what we call gamblers fallacy. The term independent has a very specific meaning in statistics, which is somewhat different from the common usage of the term. Statistical independence between two variables means that knowing the value of one variable doesnt tell us anything about the value of the other. This can be expressed as: P(A|B)=P(A). That is, the probability of A given some value of B is just the same as the overall probability of A (because they are independent). Again, while independence in common language often refers to sets that are exclusive, statistical independence refers to the case where one cannot predict anything about one variable from the value of another variable. For example, knowing a persons hair color is unlikely to tell you whether they prefer chocolate or strawberry ice cream. Later in the book we will discuss statistical tools that will let us directly test whether two variables are independent. Sometimes we want to quantify the relation between probabilities more directly, which we can do by converting them into odds which express the relative likelihood of something happening or not. We can use odds to compare different probabilities, by computing what is called an odds ratio  which is exactly what it sounds like. For example, lets say that we want to know how much the positive test increases the individuals odds of having cancer. We can first compute the prior odds  that is, the odds before we knew that the person had tested positively. An odds ratio is an example of what we will later call an effect size, which is a way of quantifying how relatively large any particular statistical effect is. A final point relates to how probabilities have been interpreted. Historically, there have been two different ways that probabilities have been interpreted. The first (known as the frequentist interpretation) interprets probabilities in terms of long-run frequencies. For example, in the case of a coin flip, it would reflect the relative frequencies of heads in the long run after a large number of flips. While this interpretation might make sense for events that can be repeated many times like a coin flip, it makes less sense for events that will only happen once, like an individual persons life or a particular presidential election; and as the economist John Maynard Keynes famously said, In the long run, we are all dead. The other interpretation of probabilities (known as the Bayesian interpretation) is as a degree of belief in a particular proposition. If I were to ask you How likely is it that the US will return to the moon by 2040, you can provide an answer to this question based on your knowledge and beliefs, even though there are no relevant frequencies to compute a frequentist probability. One way that we often frame subjective probabilities is in terms of ones willingness to accept a particular gamble. For example, if you think that the probability of the US landing on the moon by 2040 is 0.1 (i.e. odds of 9 to 1), then that means that you should be willing to accept a gamble that would pay off with anything more than 9 to 1 odds if the event occurs. As we will see, these two different definitions of probability are very relevant to the two different ways that statisticians think about testing statistical hypotheses, which we will encounter in later chapters. The fine print calculating Steph Currys free throws probability: P(2;4,0.91)=(42)0.912(10.91)42=0.040  Fine print: To determine this, we could use the binomial probability equation and plug in all of the possible values of outcomes and add them together: P(k2)=P(k=2)+P(k=1)+P(k=0)=6e5+.002+.040=.043 P(k)= P(k=2) + P(k=1) + P(k=0) = 6e^{-5} + .002 + .040 = .043 "],["recap.html", "2 Recap 2.1 Learning objectives 2.2 Exercises", " 2 Recap Probability is a mathematical tool used to study randomness. It deals with the chance (the likelihood) of an event occurring. The theory of probability began with the study of games of chance such as poker. Predictions take the form of probabilities. To predict the likelihood of an earthquake, of rain, or whether you will get an A in this course, we use probabilities. Doctors use probability to determine the chance of a vaccination causing the disease the vaccination is supposed to prevent. A stockbroker uses probability to determine the rate of return on a clients investments. You might use probability to decide to buy a lottery ticket or not. In your study of statistics, you will use the power of mathematics through probability calculations to analyze and interpret your data. 2.1 Learning objectives Having read this chapter, you should be able to: Describe the sample space for a selected random experiment. Describe the law of large numbers. Describe the difference between a probability and a conditional probability Describe the relationship between z-scores and the standard unit normal table (z-table) Probability is a tough topic for everyone, but the tools it gives us are incredibly powerful and enable us to do amazing things with data analysis. They are the heart of how inferential statistics work. 2.2 Exercises In your own words, what is probability? There is a bag with 5 red blocks, 2 yellow blocks, and 4 blue blocks. If you reach in and grab one block without looking, what is the probability it is red? Under a normal distribution, which of the following is more likely? (Note: this question can be answered without any calculations if you draw out the distributions and shade properly) Getting a z-score greater than z = 2.75 Getting a z-score less than z = -1.50 The heights of women in the United States are normally distributed with a mean of 63.7 inches and a standard deviation of 2.7 inches. If you randomly select a woman in the United States, what is the probability that she will be between 65 and 67 inches tall? The heights of men in the United States are normally distributed with a mean of 69.1 inches and a standard deviation of 2.9 inches. 6. What proportion of men are taller than 6 feet (72 inches)? You know you need to score at least 82 points on the final exam to pass your class. After the final, you find out that the average score on the exam was 78 with a standard deviation of 7. How likely is it that you pass the class? What proportion of the area under the normal curve is greater than z = 1.65? Find the z-score that bounds 25% of the lower tail of the distribution. Find the z-score that bounds the top 9% of the distribution. In a distribution with a mean of 70 and standard deviation of 12, what proportion of scores are lower than 55? "],["good-resources.html", "3 Good Resources", " 3 Good Resources https://psychnerdjae.github.io/into-the-tidyverse/ Automatic Grading with RMarkdown example Git/Github for virtual learning (from this tweet) Learn-Datascience-for-Free https://allisonhorst.shinyapps.io/dplyr-learnr/ Visualizing Linear Models: An R Bag of Tricks "],["media-without-a-home-yet.html", "4 Media without a home yet 4.1 Visualizing Linear Models: An R Bag of Tricks 4.2 For new programmers learning keyboard shortcuts 4.3 Are you a student? If yes, this is the best data science project for you! 4.4 rstudio is magic 4.5 automation quote 4.6 How computer memory works! 4.7 Is Coding a Math Skill or a Language Skill? Neither? Both? 4.8 Quantum Computers Explained! 4.9 The Rise of the Machines  Why Automation is Different this Time 4.10 Who Would Be King of America if George Washington had been made a monarch? 4.11 Emergence  How Stupid Things Become Smart Together 4.12 The Birthday Paradox 4.13 Why cant you divide by zero? 4.14 Yea hes chewing up my stats homework but that face though 4.15 Coding Kitty 4.16 Democratic databases: science on GitHub 4.17 Ten simple rules for getting started on Twitter as a scientist 4.18 NYT data ethics stuff 4.19 ", " 4 Media without a home yet 4.1 Visualizing Linear Models: An R Bag of Tricks I&#39;m starting a 3-week #rstats short course, Visualizing Linear Models: An R Bag of Tricks.One week on univariate models, two weeks on models for multivariate responses. Lectures notes, examples and exercises are at: https://t.co/LF1iVPZOPs&mdash; Michael Friendly (@datavisFriendly) February 27, 2021 4.2 For new programmers learning keyboard shortcuts https://www.shortcutfoo.com/ 4.3 Are you a student? If yes, this is the best data science project for you! 4.4 rstudio is magic Multiple cursors in @RStudio are so handy! Holding down the option key and drag gives me multiple synced cursors  pic.twitter.com/nQKzqIwsou&mdash; Emil Hvitfeldt (@Emil_Hvitfeldt) February 2, 2021 4.5 automation quote &quot;Ive always objected to doing anything over again if I had already done it once.&quot;  Grace Hopper&mdash; Programming Wisdom (@CodeWisdom) February 8, 2021 4.6 How computer memory works! 4.7 Is Coding a Math Skill or a Language Skill? Neither? Both? 4.8 Quantum Computers Explained! 4.9 The Rise of the Machines  Why Automation is Different this Time 4.10 Who Would Be King of America if George Washington had been made a monarch? 4.11 Emergence  How Stupid Things Become Smart Together 4.12 The Birthday Paradox 4.13 Why cant you divide by zero? 4.14 Yea hes chewing up my stats homework but that face though Yea hes chewing up my stats homework but that face though from r/CatsBeingCats 4.15 Coding Kitty https://hostrider.com/ 4.16 Democratic databases: science on GitHub Nature: Democratic databases: science on GitHub (Perkel, 2016). 4.17 Ten simple rules for getting started on Twitter as a scientist https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1007513 4.18 NYT data ethics stuff https://www.nytimes.com/2021/01/31/technology/facial-recognition-photo-tool.html 4.19 Art! https://t.co/XuDToJAmnp&mdash; Prof. S. Mason Garrison  (@SMasonGarrison) March 18, 2021 "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
